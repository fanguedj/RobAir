RobAIR ROS nodes for autonomous moving in crowded museum
=======================================================

This package provides several nodes used to enable the [RobAIR Configuration 2](http://air.imag.fr/mediawiki/index.php/RobAIR_2013_Configuration_2) machine to find its way through a map, while avoiding collision with surrounding people.
Its the git repository used by Ensimag team (http://fablab.ensimag.fr/index.php/RobAIR/ProjetSpe_Contribution_RobAIR)

Available nodes
---------------

Warning: work in progress.

*   **ArduinoSensors**    
    This node reads data from the sensors that are connected to the Arduino One board. These include:
    * 3 infrared proximity sensors, used to detect holes in the ground in front of the robot's wheels (typically a stairway). Data is boolean.
    * 4 up to 8 ultrasound proximity sensors, used to detect obstacles around the robot. Data is a distance expressed in centimeters.
    
    This node publishes data to two ROS topics, one for each kind of sensor: **/sensor/infrared_potholes**
    and **/sensor/ultrasound_obstacles**

*   **MotionControl**      
    This node listens to the **/cmd** topic for movement commands
    (as "move 1 meter forward" or "turn 25' left"), turns them into hardware-level instructions
    and forwards them to the motors.
    It is also responsible for "reflex behavior", which consists of stopping
    the motors in case of holes in the ground or if a person is too close to the robot.
    It does so by subscribing to the two previous topics.

*   **Keyboard**    
    Enables simple motor commands using the keyboard.
    
*   **SLAM**
    There some launch files in the launch directory. Slam algo are Hector SLAM or Gmapping, we recommend to use Gmapping to take advantage of the odometry information of the wheels.
    Once the map is correct, you can record it, to use it in the navigation program.
         $ rosrun map_server map_saver -f maps/mymaps

* **Navigation**
    The navigation program can be run using the run.sh script, which do only a chmod and launch a ros launch file (completNavigation.launch).
    It launch the navigation stack, AMCL node, "pilotage nodes", and web server.

Node and topics names follow [recommendations from the RICM 5 team](http://air.imag.fr/mediawiki/index.php/RobAIR2013-RICM5-Suivi#Architecture_ROS_d.C3.A9taill.C3.A9e) when possible.

TODO
----

* Improve the AutoPilot node.
* Make use of the skeleton tracking nodes in the above algortihms, and integrate them in the current framework.
* Improve GUI on the remote tablet
* Improve the voice node
* Make use of the reservation plateform [reservation from RCIM5] (http://air.imag.fr/mediawiki/index.php/RobAIR2013-RICM5-Suivi#Portail_de_r.C3.A9servation)
* Make use of Kinect in addition of LIDAR [transformation from pointcloud to laserscan](http://fablab.ensimag.fr/index.php/RobAIR/SLAM#S.C3.A9ance_du_17.2F04.2F2013)

Launching nodes
---------------

In a terminal, run `make`.
Make sure script files are executable with `chmod +x scripts`. 
You can then use the `./run.sh` script to automatically launch the "navigation program"
or use launch files in the launch  directory


    
